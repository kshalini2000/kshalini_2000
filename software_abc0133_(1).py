# -*- coding: utf-8 -*-
"""Software_ABC0133 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K4XMGLbSIQH-89Chu4mMAJZ_2_u2Np6G
"""

!pip install bertopic

!pip install vaderSentiment

"""# **Importing required libraries**"""

# Importing libraries
import pandas as pd
import numpy as np
import re
import unicodedata
from tqdm.auto import tqdm
import spacy
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from bertopic import BERTopic
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
from tqdm.auto import tqdm
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.feature_extraction.text import TfidfVectorizer

# Dataset loading
CSV_PATH = "esg_documents_for_dax_companies.csv"

df = pd.read_csv(
    CSV_PATH,
    sep="|",
    dtype=str,
    na_values=["", "NA", "NaN", None],
    keep_default_na=False,
    low_memory=False,
)

print(df.shape)
df.head()

# Checking data shape
print("Shape:", df.shape)

# Dataset description
df.describe()

"""# **Data preprocessing**"""

# Checking and Handling Missing Values
# Droping rows with missing or empty 'content'
before = len(df)
df = df.dropna(subset=["content"])
df = df[df["content"].str.strip().str.len() > 0]
print(f"Dropped {before - len(df)} rows with missing/empty content.")

# Checking and Removing Duplicates
before = len(df)
df = df.drop_duplicates(subset=["company", "content"])
print(f"Dropped {before - len(df)} duplicate rows based on (company, content).")

df = df.reset_index(drop=True)

# Cleaning Text data
def clean_text(s: str) -> str:
    s = str(s)
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"http\S+|www\.\S+", " ", s)
    s = re.sub(r"\S+@\S+", " ", s)
    s = re.sub(r"[^a-zA-Z\s]", " ", s)
    s = s.lower()
    s = re.sub(r"\s+", " ", s).strip()
    return s

tqdm.pandas(desc="cleaning")
df["clean_text"] = df["content"].progress_apply(clean_text)

# Tokenization & Lemmatization
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
# Increase the max_length to handle potentially long documents
nlp.max_length = 3_000_000

tokens_list, lemmas_list = [], []

# Process the text in batches to reduce memory usage
for doc in tqdm(nlp.pipe(df["clean_text"].tolist(), batch_size=500), total=len(df), desc="spaCy"):
    tokens = [t.text for t in doc if t.is_alpha and not t.is_stop]
    lemmas = [t.lemma_.lower() for t in doc if t.is_alpha and not t.is_stop]
    tokens_list.append(tokens)
    lemmas_list.append(lemmas)


df["tokens"] = tokens_list
df["lemmas"] = lemmas_list
df["num_tokens"] = df["tokens"].map(len)

# Filtering Very Short Docs
min_tokens = 10
before = len(df)
df = df[df["num_tokens"] >= min_tokens].reset_index(drop=True)
print(f"Removed {before - len(df)} very short docs (<{min_tokens} tokens).")

# Saving Preprocessed Dataset
df.to_parquet("esg_preprocessed.parquet", index=False)
print("Preprocessing complete. Saved as 'esg_preprocessed.parquet'.")

"""# **Exploratory Data Analysis**"""

# Document Trend Over Time
df["date"] = pd.to_datetime(df["date"], errors="coerce")
docs_over_time = df.groupby(df["date"].dt.to_period("M")).size()

docs_over_time.plot(figsize=(12,5))
plt.title("Documents Over Time (Monthly)")
plt.ylabel("Count")
plt.xlabel("Date")
plt.show()

# Distribution of ESG Topics
topic_counts = df["esg_topics"].value_counts().head(15)
topic_counts.plot(kind="bar", figsize=(10,5))
plt.title("Most Common ESG Topics")
plt.ylabel("Count") # Change ylabel back for vertical bar chart
plt.xlabel("ESG Topics") # Change xlabel back for vertical bar chart
plt.xticks(rotation=90) # Rotate labels for better readability
plt.show()

# Top Words Cloud
text = " ".join(df["clean_text"].dropna().tolist())
wc = WordCloud(width=800, height=400, background_color="white").generate(text)

plt.figure(figsize=(12,6))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of ESG Content")
plt.show()

# Token Count vs Company
avg_len_by_company = df.groupby("company")["num_tokens"].mean().sort_values(ascending=False).head(15)
avg_len_by_company.plot(kind="barh", figsize=(10, 5))
plt.title("Average Document Length per Company")
plt.xlabel("Avg Tokens")
plt.ylabel("Company")
plt.gca().invert_yaxis() # Invert y-axis to show the highest average length at the top
plt.show()

# Datatype Distribution
df["datatype"].value_counts().plot(kind="bar", figsize=(8,4))
plt.title("Distribution of Document Types")
plt.ylabel("Count")
plt.show()

"""# **Natural Language Processing**"""

# Topic Modeling with LDA
# Vectorization for LDA
count_vectorizer = CountVectorizer(max_features=5000, stop_words='english')
X_counts = count_vectorizer.fit_transform(df["clean_text"])

# LDA model
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X_counts)

# Displaying top words per topic
terms = count_vectorizer.get_feature_names_out()
for idx, topic in enumerate(lda.components_):
    top_terms = [terms[i] for i in topic.argsort()[-10:]]
    print(f"Topic {idx+1}: {' | '.join(top_terms)}")

# Topic Modeling with BERTopic
topic_model = BERTopic(language="english", verbose=True)
topics, probs = topic_model.fit_transform(df["clean_text"])

# Showing top topics
topic_model.get_topic_info().head(10)

# Sentiment Analysis (VADER + TextBlob)
analyzer = SentimentIntensityAnalyzer()

def analyze_sentiment_optimized(text):
    # Ensuring text is a string, handling potential NaN or non-string types
    text = str(text)[:512]
    vader_score = analyzer.polarity_scores(text)["compound"]
    textblob_score = TextBlob(text).sentiment.polarity
    return vader_score, textblob_score

# Applying the optimized function
tqdm.pandas(desc="Analyzing Sentiment")
df[["vader_score", "textblob_score"]] = df["clean_text"].progress_apply(analyze_sentiment_optimized).tolist()


# Classifying sentiment (positive / neutral / negative)
df["sentiment_label"] = df["vader_score"].apply(
    lambda x: "positive" if x > 0.05 else ("negative" if x < -0.05 else "neutral")
)

display(df[["clean_text", "vader_score", "textblob_score", "sentiment_label"]].head())

# Visualising Sentiment Distribution
sentiment_counts = df["sentiment_label"].value_counts()

plt.figure(figsize=(7,5))
sentiment_counts.plot(kind="bar")
plt.title("Distribution of Sentiment Labels")
plt.xlabel("Sentiment")
plt.ylabel("Number of Documents")
plt.xticks(rotation=0)
plt.show()

# Clustering (K-Means + Hierarchical)

# TF-IDF for clustering
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X_tfidf = tfidf.fit_transform(df["clean_text"])

# K-Means clustering
kmeans = KMeans(n_clusters=5, random_state=42)
df["kmeans_cluster"] = kmeans.fit_predict(X_tfidf)

# Hierarchical clustering
hier = AgglomerativeClustering(n_clusters=5)
df["hier_cluster"] = hier.fit_predict(X_tfidf.toarray())

# Validation (Cross-check Sentiment Consistency)
# Correlation between VADER & TextBlob
correlation = np.corrcoef(df["vader_score"], df["textblob_score"])[0,1]
print("Correlation between VADER and TextBlob sentiment scores:", correlation)